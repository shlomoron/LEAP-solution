# LEAP-solution
My solution for the Kaggle competition of [LEAP (ClimSim)](https://www.kaggle.com/competitions/leap-atmospheric-physics-ai-climsim).  

# The steps to reproduce my solution  
## 1. Creating the datasets
### For the low-resolution dataset:
1.1. Download the data: see [here](https://www.kaggle.com/code/shlomoron/leap-download-data-1) a notebook that download 1/32 of the data. This need to be repeated 32 times by changing the index from 0 to any integer up to 31 in this line:  
'files = [all_files[i] for i in range(0, len(all_files), 32)]'  
For example, for index=5 this line will read:  
'files = [all_files[i] for i in range(5, len(all_files), 32)]'  
You can acsess all the notebooks by changing the index of the notebook in the link (the number at the end of the link).  
1.2. Encode the downloaded data to TFRecords. See notebook [here](https://www.kaggle.com/code/shlomoron/leap-data-to-tfrecs-1-s). This notebook also need to be repeated 32 times. You can acsess all 32 notebooks by changing the index in the link.  
1.3. Create kaggle datasets of TFRecords by combining the output of the notebooks from 1.2. Each dataset is created from 4 notebooks, for a total of 8 dataset. See dataset creation notebook [here]([https://www.kaggle.com/code/shlomoron/leap-tfrec-combined-1-s/notebook](https://www.kaggle.com/code/shlomoron/leap-tfrec-combined-1-s-public).  This time I only give a copy (not the original notebook) since this notebook print username and Kaggle key, the latter should not be made public. In any case the dataset creation notebook cannot by directly linked to the dataset (if I want I can delete the dataset and create a new one with the same name from another, different notebook) so if you want to make sure I did not hide any leak in the dataset, you will have to check it manually or recreate it with this notebook (you will need to provide your own 'kaggle.json' with username and key and run 8 copies of this notebook, with each copy combining 4 notebooks from 1.2).The dataset is public and you can see it [here](https://www.kaggle.com/datasets/shlomoron/leap-tfrecs-combined-1-s-ds). 
### For the high-resolution dataset:  
When I created the high-res dataset I already had the experience from the low-res one, so bullets 1.1-1.3 are combined into a single notebook. Here too I give a copy and not the original one, from the same reason I mentioned in 1.3. See the notebook [here](https://www.kaggle.com/code/shlomoron/leap-download-data-1-xx-public). I run 44 copies of this notebook, with notebook_id between 1 to 44. Each notebook download 5[batches]\*100[grids]\*21600[atmospheric columns] for a total of 10,800,000 samples per notebook resulting in a dataset of \~60GB in size per notebook (and \~60*44~2.6TB in total. I wanted to download more but started to get afraid that Kaggle system decide that I'm a bot and ban me, lol. Also, each high-res model I train use approximately only 0.5TB or so of this data so it's enough for the numbers of models I trained). See the first dataset [here](https://www.kaggle.com/datasets/shlomoron/leap-tfrecs-1-xx). 
## 2. Create auciliary data sets  
There are several datasets that I need for training.  
2.1. Grid metadata. This is for latitude/longtitude data of the grid indices, since I use lat/lon as auxiliary loss. See notebook [here](https://www.kaggle.com/code/shlomoron/leap-grid) and dataset [here](https://www.kaggle.com/datasets/shlomoron/leap-gdata-ds).  
2.2. Mean, std and min/max of all columns in Kaggle train and test set. Well, it slightly more complicated since for features that differ over the 60 levels I calculate also mean/std/min/max for all the levels combined, and is separate between 60-levels features (cols) and the features that are the same for all 60 levels (col_not). If it was not clear enough, I hope it will be after you read the code. I need mean/std data for normalization during training: (x-mean(x))/std(x), and min/max are needed for log and soft clipping of the feature. Here too I dont give the original notebook, since it is messy. Instead, [here](https://www.kaggle.com/datasets/shlomoron/leap-msm-ds) is the dataset, [here](https://www.kaggle.com/code/shlomoron/leap-msm-public) is a clean version of a notebook to create this dataset, and [here](https://www.kaggle.com/code/shlomoron/leap-msm-compare) is a notebook showing that the given notebook output the same values as in the dataset.  
2.3. Absolute min/max values for the targets. In 2.2 I calculaed min/max over the Kaggle data, but later on when i performed soft clipping on the high-res targets, I wanted to be more precise and use the absolute min/max over the complete HF low-res dataset. I fund these values [here](https://www.kaggle.com/code/shlomoron/leap-y-minmax). I save them to dataset in the next bullet (2.4).  
2.4. The normalization values of sample_submission, both old and new. I called them 'stds' since the old ones were 1/std. They arte saved to a dataset [in this notebook](https://www.kaggle.com/code/shlomoron/leap-sample-submission-stds), which is also used to save to the same dataset the min/max from 2.3 (the dataset was already in my pipeline at his point so I just piggybacked on it). The dataset is [here](https://www.kaggle.com/datasets/shlomoron/leap-sample-submission-stds-ds).  
## 3. Find gcp path for the TFRecords data.
Once upon a time, when the TPU instances in Kaggle were the old (decaprated) ones, one would have to feed the data through a GCP bucket of TFRecords. Luckily, publick Kaggle dataset are stored on GCP buckets and can be accesses directly using GCP path. Nowadays its not necessary anymore on Kaggle, however...when training on massive amount on data it's still preferable, since attaching ~3TB to a kaggle notebook can take a very very very long time to download. On a side note, if you are using Colab it's still the easiest way regardless of dataset size. There is a cavat-   
